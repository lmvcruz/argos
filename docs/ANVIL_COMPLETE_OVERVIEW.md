# Anvil Data Handling - Complete Overview

## Quick Summary

**Anvil** parses tool outputs and stores the results. **Verdict** validates the parsing by comparing actual output against expected output. Together, they form a validation pipeline.

```
Tool Output â†’ Anvil Parser â†’ Parsed Data â†’ Verdict Validator â†’ Pass/Fail
                                  â†“
                           SQLite Database
                                  â†“
                              REST API
                                  â†“
                           React Frontend
```

---

## The Three Core Questions Answered

### 1. How does Anvil save the data to the database?

**Flow**: Parse â†’ Convert to StorageModel â†’ Insert into SQLite

```python
# Parsing
parser = LintParser()
lint_data = parser.parse_flake8_output(raw_output, Path("."))
# Result: LintData object with parsed information

# Storage
persistence = StatisticsPersistence(db)
persistence.save_lint_summary(lint_data)
persistence.save_lint_violations(lint_data)
# Result: Data inserted into SQLite tables
```

**Database Tables Created**:
- `validation_runs` - Overall run metadata
- `test_case_records` - Individual test results
- `lint_summary` - Aggregated lint results
- `lint_violations` - Detailed violation records
- `coverage_summary` - Coverage statistics
- `validator_run_records` - Validator execution results
- `file_validation_records` - Per-file results

### 2. How can we visualize the parsed data?

**Three methods**:

1. **Direct Query** (Python)
   ```python
   db = StatisticsDatabase(".anvil/execution.db")
   query = StatisticsQueryEngine(db)
   violations = query.get_lint_violations(run_id=1)
   ```

2. **REST API** (from Lens backend)
   ```
   GET /api/validation/runs
   GET /api/validation/runs/{id}/lint
   GET /api/validation/runs/{id}/coverage
   ```

3. **Frontend** (React component)
   ```typescript
   const [data, setData] = useState([]);
   useEffect(() => {
     fetch('/api/validation/runs/{id}/lint')
       .then(res => res.json())
       .then(setData);
   }, []);
   ```

### 3. How does Verdict call Anvil to get parsed data?

**Flow**: Test â†’ Adapter â†’ Parser â†’ Dict â†’ Comparison

```
Test Case (e.g., black_single_file.yaml)
    â†“
Read input file (e.g., black_output.txt)
    â†“
Call Verdict adapter:
    validate_black_parser(input_text)
    â†“ [Inside adapter]
    Create parser: LintParser()
    Parse: lint_data = parser.parse_black_output(input_text, Path("."))
    Convert: return dict(lint_data)
    â†“ [Back to Verdict]
Get actual output (dict)
    â†“
Read expected output file (expected_output.yaml)
    â†“
Compare: OutputValidator.validate(actual, expected)
    â†“
Result: PASS or FAIL
```

---

## Architecture Components

### Anvil Components

```
anvil/
â”œâ”€â”€ parsers/                    # Parse tool outputs
â”‚   â”œâ”€â”€ lint_parser.py         # Black, flake8, isort
â”‚   â”œâ”€â”€ pytest_parser.py       # Pytest results
â”‚   â””â”€â”€ coverage_parser.py     # Coverage data
â”‚
â”œâ”€â”€ storage/                    # Store parsed data
â”‚   â”œâ”€â”€ statistics_database.py    # SQLite schema
â”‚   â”œâ”€â”€ statistics_persistence.py # Save to DB
â”‚   â””â”€â”€ statistics_queries.py     # Query from DB
â”‚
â”œâ”€â”€ validators/
â”‚   â””â”€â”€ adapters.py             # Convert parsed data to dicts
â”‚
â””â”€â”€ executors/                  # Run tests with history
    â””â”€â”€ pytest_executor.py      # Pytest with database recording
```

### Verdict Components

```
verdict/
â”œâ”€â”€ runner.py              # Orchestrate test execution
â”œâ”€â”€ executor.py            # Call adapter functions
â”œâ”€â”€ validator.py           # Compare actual vs expected
â”œâ”€â”€ loader.py              # Load test cases
â””â”€â”€ cli.py                 # Command-line interface
```

### Lens Components

```
lens/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ server.py          # FastAPI REST endpoints
â”‚
â””â”€â”€ frontend/
    â””â”€â”€ src/
        â””â”€â”€ pages/
            â””â”€â”€ ValidationResults.tsx  # React component
```

---

## Data Models

### LintData (from Anvil Parser)

```python
@dataclass
class LintData:
    validator: str                    # "black", "flake8", "isort"
    total_violations: int             # Total count
    errors: int                       # ERROR severity count
    warnings: int                     # WARNING severity count
    info: int                         # INFO severity count
    files_scanned: int                # Number of files
    by_code: Dict[str, int]          # Count by violation code
    file_violations: List[FileViolation]  # Per-file details
```

### ValidationRun (in Database)

```python
@dataclass
class ValidationRun:
    id: int
    timestamp: str                    # ISO format
    git_commit: str
    git_branch: str
    incremental: bool
    passed: bool                      # Overall status
    duration_seconds: float
```

### TestCaseRecord (in Database)

```python
@dataclass
class TestCaseRecord:
    id: int
    run_id: int                       # Foreign key to validation_run
    test_name: str
    test_suite: str
    passed: bool
    skipped: bool
    duration_seconds: float
    failure_message: Optional[str]
```

### LintSummary (in Database)

```python
@dataclass
class LintSummary:
    id: int
    run_id: int                       # Foreign key to validation_run
    validator: str                    # "black", "flake8", etc.
    files_scanned: int
    total_violations: int
    errors: int
    warnings: int
    info: int
    by_code: Dict[str, int]          # JSON stored in DB
```

---

## Complete Example: Black Parser Flow

### Input
```
Raw black output:
error: cannot format module: Black does not support Python 3.7
Oh no! ðŸ’¥ ðŸ’” ðŸ’¥
error: 2 files failed to reformat.
```

### Step 1: Parse (Anvil)
```python
parser = LintParser()
parsed = parser.parse_black_output(input_text, Path("."))
# Result: LintData(validator="black", total_violations=2, errors=2, ...)
```

### Step 2: Store (Anvil)
```python
persistence = StatisticsPersistence(db)
persistence.save_lint_summary(parsed)
persistence.save_lint_violations(parsed)
# Tables updated: lint_summary, lint_violations
```

### Step 3: Adapt (Anvil)
```python
def validate_black_parser(input_text: str) -> dict:
    parser = LintParser()
    parsed = parser.parse_black_output(input_text, Path("."))
    return {
        "validator": "black",
        "total_violations": 2,
        "errors": 2,
        "warnings": 0,
        "info": 0,
        "by_code": {"E901": 2},
        ...
    }
```

### Step 4: Execute (Verdict)
```python
actual = validate_black_parser(input_text)
# Returns: {"validator": "black", "total_violations": 2, ...}
```

### Step 5: Validate (Verdict)
```python
expected = {"validator": "black", "total_violations": 2, ...}
is_valid, diffs = validator.validate(actual, expected)
# Result: is_valid = True, diffs = []
```

### Step 6: Query (Lens Backend)
```python
query = StatisticsQueryEngine(db)
violations = query.get_lint_violations(run_id=1)
# Returns: [LintViolation(...), LintViolation(...)]
```

### Step 7: Visualize (Lens Frontend)
```
Validation Run #1
â”œâ”€ Branch: main
â”œâ”€ Status: FAIL
â””â”€ Lint Results
   â””â”€ black: 2 violations (2 errors, 0 warnings)
```

---

## Database Schema (Simplified)

```sql
-- Run metadata
validation_runs
â”œâ”€ id INTEGER PRIMARY KEY
â”œâ”€ timestamp TEXT
â”œâ”€ git_commit TEXT
â”œâ”€ git_branch TEXT
â”œâ”€ passed INTEGER (0/1)
â””â”€ duration_seconds REAL

-- Test results
test_case_records
â”œâ”€ id INTEGER PRIMARY KEY
â”œâ”€ run_id INTEGER (FK to validation_runs)
â”œâ”€ test_name TEXT
â”œâ”€ test_suite TEXT
â”œâ”€ passed INTEGER (0/1)
â””â”€ duration_seconds REAL

-- Lint aggregates
lint_summary
â”œâ”€ id INTEGER PRIMARY KEY
â”œâ”€ run_id INTEGER (FK to validation_runs)
â”œâ”€ validator TEXT
â”œâ”€ total_violations INTEGER
â”œâ”€ errors INTEGER
â”œâ”€ warnings INTEGER
â””â”€ by_code TEXT (JSON)

-- Lint details
lint_violations
â”œâ”€ id INTEGER PRIMARY KEY
â”œâ”€ run_id INTEGER (FK to validation_runs)
â”œâ”€ file_path TEXT
â”œâ”€ line_number INTEGER
â”œâ”€ severity TEXT
â”œâ”€ code TEXT
â””â”€ message TEXT

-- Coverage
coverage_summary
â”œâ”€ id INTEGER PRIMARY KEY
â”œâ”€ run_id INTEGER (FK to validation_runs)
â”œâ”€ total_coverage REAL
â”œâ”€ files_analyzed INTEGER
â””â”€ covered_statements INTEGER

-- Validators
validator_run_records
â”œâ”€ id INTEGER PRIMARY KEY
â”œâ”€ run_id INTEGER (FK to validation_runs)
â”œâ”€ validator_name TEXT
â”œâ”€ passed INTEGER (0/1)
â”œâ”€ error_count INTEGER
â””â”€ warning_count INTEGER

-- Per-file validation
file_validation_records
â”œâ”€ id INTEGER PRIMARY KEY
â”œâ”€ run_id INTEGER (FK to validation_runs)
â”œâ”€ file_path TEXT
â”œâ”€ validator_name TEXT
â”œâ”€ error_count INTEGER
â””â”€ warning_count INTEGER
```

---

## API Endpoints (Lens Backend)

```
GET /api/validation/runs
  â””â”€ Returns: List[ValidationRun]

GET /api/validation/runs/{run_id}
  â””â”€ Returns: ValidationRun

GET /api/validation/runs/{run_id}/tests
  â””â”€ Returns: List[TestCaseRecord]

GET /api/validation/runs/{run_id}/lint
  â””â”€ Returns: {summary: List[LintSummary], violations: List[LintViolation]}

GET /api/validation/runs/{run_id}/coverage
  â””â”€ Returns: List[CoverageSummary]

GET /api/validation/runs/{run_id}/validators
  â””â”€ Returns: List[ValidatorRunRecord]

GET /api/validation/runs/{run_id}/files
  â””â”€ Returns: List[FileValidationRecord]
```

---

## Usage Patterns

### Pattern 1: Real-time Parsing
Use when you need immediate results without storage.

```python
from anvil.parsers.lint_parser import LintParser

parser = LintParser()
result = parser.parse_flake8_output(tool_output, Path("."))
# Use result directly, don't store
```

### Pattern 2: Parse and Store
Use when you want to keep historical records.

```python
from anvil.parsers.lint_parser import LintParser
from anvil.storage.statistics_database import StatisticsDatabase
from anvil.storage.statistics_persistence import StatisticsPersistence

parser = LintParser()
result = parser.parse_flake8_output(tool_output, Path("."))

db = StatisticsDatabase(".anvil/execution.db")
persistence = StatisticsPersistence(db)
persistence.save_lint_summary(result)
```

### Pattern 3: Validate with Verdict
Use when you want to validate against expected output.

```python
from anvil.validators.adapters import validate_flake8_parser
from verdict.validator import OutputValidator

actual = validate_flake8_parser(tool_output)
expected = load_expected_output()

validator = OutputValidator()
is_valid, diffs = validator.validate(actual, expected)
```

### Pattern 4: Query Historical Data
Use when you want to analyze past results.

```python
from anvil.storage.statistics_database import StatisticsDatabase
from anvil.storage.statistics_queries import StatisticsQueryEngine

db = StatisticsDatabase(".anvil/execution.db")
query = StatisticsQueryEngine(db)

runs = query.get_validation_runs(limit=10)
for run in runs:
    violations = query.get_lint_violations(run_id=run.id)
    print(f"Run {run.id}: {len(violations)} violations")
```

### Pattern 5: REST API Access
Use when you want to access from web/frontend.

```javascript
// Frontend code
const response = await fetch('/api/validation/runs/1/lint');
const data = await response.json();
console.log(`Found ${data.violations.length} violations`);
```

---

## Key Files Reference

| File | Purpose |
|------|---------|
| `anvil/parsers/lint_parser.py` | Parse black, flake8, isort output |
| `anvil/parsers/pytest_parser.py` | Parse pytest output |
| `anvil/storage/statistics_database.py` | Database schema & CRUD |
| `anvil/storage/statistics_persistence.py` | Save parsed data to DB |
| `anvil/storage/statistics_queries.py` | Query parsed data from DB |
| `anvil/validators/adapters.py` | Convert parsed data to dicts |
| `verdict/executor.py` | Execute adapter functions |
| `verdict/validator.py` | Compare actual vs expected |
| `verdict/runner.py` | Orchestrate test execution |
| `lens/backend/server.py` | REST API endpoints |
| `lens/frontend/src/pages/` | React components |

---

## Decision Tree: Which Method to Use?

```
Do you have raw tool output?
â”œâ”€ YES, need parsed data NOW
â”‚  â””â”€ Use: LintParser.parse_*() directly
â”‚
â”œâ”€ YES, need to store for later
â”‚  â””â”€ Use: LintParser.parse_*() + StatisticsPersistence.save_*()
â”‚
â””â”€ YES, need to validate correctness
   â””â”€ Use: Verdict adapters + OutputValidator

Do you have stored parsed data?
â”œâ”€ YES, need to retrieve it
â”‚  â””â”€ Use: StatisticsQueryEngine.get_*()
â”‚
â””â”€ YES, need to display it
   â””â”€ Use: REST API or direct query

Do you have a running Lens backend?
â”œâ”€ YES
â”‚  â””â”€ Use: Fetch from /api/validation/* endpoints
â”‚
â””â”€ NO
   â””â”€ Use: Direct Python StatisticsQueryEngine
```

---

## Summary Table

| Layer | Component | Input | Processing | Output |
|-------|-----------|-------|-----------|--------|
| **Parse** | LintParser | Raw string (tool output) | Parse & extract | LintData object |
| **Adapt** | Adapters | Raw string | Call parser + convert | Python dict |
| **Validate** | OutputValidator | Actual dict, Expected dict | Compare | bool + differences |
| **Store** | Persistence | LintData object | Insert SQL | Rows in database |
| **Query** | QueryEngine | Query parameters | SQL SELECT | List of models |
| **Expose** | REST API | HTTP request | Query + format | JSON response |
| **Display** | React | Fetch response | Process & render | Web UI |

---

## Conclusion

Anvil provides a complete pipeline for:
1. **Parsing** tool outputs into structured data
2. **Storing** parsed data in a queryable database
3. **Validating** parsing correctness with Verdict
4. **Exposing** data via REST API
5. **Visualizing** results in a web interface

Choose the layer that fits your use case, or combine multiple layers for a complete solution!
