"""
Coverage parser for pytest-cov XML output.

This module provides functionality to parse coverage.xml files generated by
pytest-cov and extract coverage metrics for storage in the Anvil database.
"""

import xml.etree.ElementTree as ET
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional


@dataclass
class FileCoverage:
    """
    Coverage data for a single file.

    Args:
        file_path: Relative path to the file
        total_statements: Total number of executable statements
        covered_statements: Number of covered statements
        coverage_percentage: Coverage percentage (0.0-100.0)
        missing_lines: List of uncovered line numbers
    """

    file_path: str
    total_statements: int
    covered_statements: int
    coverage_percentage: float
    missing_lines: List[int]


@dataclass
class CoverageData:
    """
    Complete coverage data from a coverage run.

    Args:
        total_coverage: Overall coverage percentage (0.0-100.0)
        files_analyzed: Number of files analyzed
        total_statements: Total statements across all files
        covered_statements: Total covered statements across all files
        file_coverage: List of per-file coverage data
    """

    total_coverage: float
    files_analyzed: int
    total_statements: int
    covered_statements: int
    file_coverage: List[FileCoverage]


class CoverageParser:
    """
    Parse pytest-cov coverage.xml output.

    Examples:
        >>> parser = CoverageParser()
        >>> data = parser.parse_coverage_xml("coverage.xml")
        >>> print(f"Total coverage: {data.total_coverage:.2f}%")
        Total coverage: 94.50%
    """

    def parse_coverage_xml(self, xml_path: str) -> CoverageData:
        """
        Parse coverage.xml file generated by pytest-cov.

        The XML format is Cobertura-style with this structure:
        <coverage>
          <packages>
            <package name="module">
              <classes>
                <class filename="path/to/file.py" line-rate="0.95">
                  <lines>
                    <line number="10" hits="1"/>
                    <line number="11" hits="0"/>
                  </lines>
                </class>
              </classes>
            </package>
          </packages>
        </coverage>

        Args:
            xml_path: Path to coverage.xml file

        Returns:
            CoverageData instance with parsed coverage information

        Raises:
            FileNotFoundError: If XML file doesn't exist
            ET.ParseError: If XML is malformed
        """
        xml_path_obj = Path(xml_path)
        if not xml_path_obj.exists():
            raise FileNotFoundError(f"Coverage file not found: {xml_path}")

        tree = ET.parse(xml_path)
        root = tree.getroot()

        # Extract overall coverage from root element
        line_rate = float(root.attrib.get("line-rate", "0"))
        total_coverage = line_rate * 100.0

        # Parse per-file coverage
        file_coverage_list = []
        total_statements_all = 0
        covered_statements_all = 0

        # Iterate through all packages and classes
        for package in root.findall(".//package"):
            for class_elem in package.findall(".//class"):
                file_path = class_elem.attrib.get("filename", "")
                class_line_rate = float(class_elem.attrib.get("line-rate", "0"))

                # Count statements and covered lines
                lines = class_elem.findall(".//line")
                total_statements = len(lines)
                covered_statements = sum(1 for line in lines if int(line.attrib.get("hits", "0")) > 0)
                missing_lines = [
                    int(line.attrib.get("number"))
                    for line in lines
                    if int(line.attrib.get("hits", "0")) == 0
                ]

                coverage_percentage = class_line_rate * 100.0

                file_coverage_list.append(
                    FileCoverage(
                        file_path=file_path,
                        total_statements=total_statements,
                        covered_statements=covered_statements,
                        coverage_percentage=coverage_percentage,
                        missing_lines=missing_lines,
                    )
                )

                total_statements_all += total_statements
                covered_statements_all += covered_statements

        return CoverageData(
            total_coverage=total_coverage,
            files_analyzed=len(file_coverage_list),
            total_statements=total_statements_all,
            covered_statements=covered_statements_all,
            file_coverage=file_coverage_list,
        )

    def calculate_coverage_diff(
        self, current: CoverageData, baseline: CoverageData
    ) -> Dict[str, float]:
        """
        Calculate coverage changes between two coverage runs.

        Args:
            current: Current coverage data
            baseline: Baseline coverage data to compare against

        Returns:
            Dictionary with coverage differences:
            - total_coverage_diff: Change in overall coverage percentage
            - files_improved: Number of files with improved coverage
            - files_regressed: Number of files with decreased coverage
            - new_files: Number of files added
            - removed_files: Number of files removed
        """
        # Create file path to coverage mapping
        current_files = {fc.file_path: fc for fc in current.file_coverage}
        baseline_files = {fc.file_path: fc for fc in baseline.file_coverage}

        files_improved = 0
        files_regressed = 0
        new_files = set(current_files.keys()) - set(baseline_files.keys())
        removed_files = set(baseline_files.keys()) - set(current_files.keys())

        # Compare common files
        for file_path in set(current_files.keys()) & set(baseline_files.keys()):
            current_cov = current_files[file_path].coverage_percentage
            baseline_cov = baseline_files[file_path].coverage_percentage

            if current_cov > baseline_cov:
                files_improved += 1
            elif current_cov < baseline_cov:
                files_regressed += 1

        return {
            "total_coverage_diff": current.total_coverage - baseline.total_coverage,
            "files_improved": files_improved,
            "files_regressed": files_regressed,
            "new_files": len(new_files),
            "removed_files": len(removed_files),
        }

    def find_coverage_regressions(
        self, current: CoverageData, baseline: CoverageData, threshold: float = 1.0
    ) -> List[Dict[str, any]]:
        """
        Identify files with coverage regressions.

        Args:
            current: Current coverage data
            baseline: Baseline coverage data
            threshold: Minimum percentage drop to consider a regression (default: 1.0%)

        Returns:
            List of dictionaries with regression information:
            - file_path: Path to file with regression
            - current_coverage: Current coverage percentage
            - baseline_coverage: Baseline coverage percentage
            - coverage_drop: Amount of coverage decrease
        """
        current_files = {fc.file_path: fc for fc in current.file_coverage}
        baseline_files = {fc.file_path: fc for fc in baseline.file_coverage}

        regressions = []

        # Check common files
        for file_path in set(current_files.keys()) & set(baseline_files.keys()):
            current_cov = current_files[file_path].coverage_percentage
            baseline_cov = baseline_files[file_path].coverage_percentage
            coverage_drop = baseline_cov - current_cov

            if coverage_drop >= threshold:
                regressions.append(
                    {
                        "file_path": file_path,
                        "current_coverage": current_cov,
                        "baseline_coverage": baseline_cov,
                        "coverage_drop": coverage_drop,
                    }
                )

        # Sort by coverage drop (largest first)
        regressions.sort(key=lambda x: x["coverage_drop"], reverse=True)

        return regressions
